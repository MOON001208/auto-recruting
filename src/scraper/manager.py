from src.config import Config
from src.scraper.saramin import SaraminScraper
from src.scraper.jobkorea import JobKoreaScraper
from src.scraper.linkareer import LinkareerScraper
from src.scraper.incruit import IncruitScraper
from src.scraper.wanted import WantedScraper

class ScraperManager:
    def __init__(self):
        self.scrapers = [
            SaraminScraper(),      # μ‚¬λμΈ
            JobKoreaScraper(),     # μ΅μ½”λ¦¬μ•„
            LinkareerScraper(),    # λ§μ»¤λ¦¬μ–΄
            IncruitScraper(),      # μΈν¬λ£¨νΈ
            WantedScraper()        # μ›ν‹°λ“
        ]

    def run_all(self):
        all_jobs = []

        # λ¨λ“  μΉ΄ν…κ³ λ¦¬μ ν‚¤μ›λ“ μμ§‘
        targets = []
        for category, keywords in Config.KEYWORDS.items():
            targets.extend(keywords)

        # μ¤‘λ³µ ν‚¤μ›λ“ μ κ±°
        targets = list(set(targets))

        print(f"π” Starting scrape for {len(targets)} keywords across {len(self.scrapers)} sites...")
        print(f"π“ Sites: μ‚¬λμΈ, μ΅μ½”λ¦¬μ•„, λ§μ»¤λ¦¬μ–΄, μ›ν‹°λ“")
        
        for scraper in self.scrapers:
            scraper_name = scraper.__class__.__name__
            try:
                print(f"\nβ–¶ Running {scraper_name}...")
                jobs = scraper.search(targets)
                print(f"  β… {scraper_name}: {len(jobs)}κ° κ³µκ³  μμ§‘")
                all_jobs.extend(jobs)
            except Exception as e:
                print(f"  β {scraper_name} failed: {e}")
        
        print(f"\nπ“ Total collected: {len(all_jobs)} jobs")
        return all_jobs
